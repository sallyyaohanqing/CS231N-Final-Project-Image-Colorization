{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Pretrain_CNN_Based_ColorizationNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained CNN-Based Image Colorization Net model"
      ],
      "metadata": {
        "id": "6bbTj2j2CDx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab file includes codes to train CNN model for Image Colorization task.\n",
        "The CNN model combines the downsampling convolutional layers from pretrained model such as ResNet18, AlexNet, and VGG16 with customized upsampling layers.\n",
        "Image data are trained using Lab channels where we use L channel as input and ab channels as target."
      ],
      "metadata": {
        "id": "1gD0lxI19aCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgement:\n",
        "The model architecture is inspired by Luke Melas-Kyriazi's blog post $\\textbf{Image Colorization with Convolutional Neural Networks}$: https://lukemelas.github.io/image-colorization.html. Parts of the codes are modified based on the codes the author shared in the blog post and Github: https://github.com/lukemelas/Automatic-Image-Colorization/."
      ],
      "metadata": {
        "id": "a99r7EeCCT00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and import libraries\n",
        "!pip install torch torchvision matplotlib numpy scikit-image pillow==4.1.1"
      ],
      "metadata": {
        "id": "aH0dC6LHNLBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/CellProfiling/HPA-Cell-Segmentation/archive/master.zip"
      ],
      "metadata": {
        "id": "ZTgtf52KPc-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow==9.0.0"
      ],
      "metadata": {
        "id": "ExoNR3xh1Rwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip file\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/231N project/tiny-imagenet-200 2.zip\", 'r')\n",
        "zip_ref.extractall(\".\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDIPLSOyi-jH",
        "outputId": "356419da-a3f4-481f-d6d3-acdb36631351"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# For conversion between RGB channel and Lab channel\n",
        "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
        "from skimage import io\n",
        "# For CNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "# For utilities\n",
        "import os, shutil, time"
      ],
      "metadata": {
        "id": "ZEimdho-NS03"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "JebMDYYcNTWt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsampling convolutional layers from Pretrained ResNet18 with customized upsampling layers\n",
        "class ResColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(ResColorizationNet, self).__init__()\n",
        "    CONV_FEATURE_SIZE = 128\n",
        "\n",
        "    ## First half: ResNet\n",
        "    resnet = models.resnet18(num_classes=365) \n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    # Extract convolutional downsampling layers architecture\n",
        "    self.conv_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(     \n",
        "      nn.Conv2d(CONV_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through ResNet-gray to extract features\n",
        "    conv_features = self.conv_resnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(conv_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "BDbj81YCU08r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsampling convolutional layers from Pretrained AlexNet with customized upsampling layers\n",
        "class AlexColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(AlexColorizationNet, self).__init__()\n",
        "    CONV_FEATURE_SIZE = 256\n",
        "\n",
        "    ## First half: Alexnet\n",
        "    alexnet = models.alexnet(num_classes=365) \n",
        "    # Extract convolutional downsampling layers architecture\n",
        "    self.conv_alexnet = nn.Sequential(*list(alexnet.children())[0][:12])\n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    self.conv_alexnet[0].weight=nn.Parameter(self.conv_alexnet[0].weight.sum(dim=1).unsqueeze(1)) \n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(  \n",
        "      nn.Conv2d(CONV_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=3),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=3),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      # nn.BatchNorm2d(32),\n",
        "      # nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(2),\n",
        "      nn.ReLU(),\n",
        "      # nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(2, 2, kernel_size=8, stride=1, padding=0),\n",
        "      nn.LeakyReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through AlexNet-gray to extract features\n",
        "    conv_features = self.conv_alexnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(conv_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "0Bu8Nz7mNZOw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsampling convolutional layers from Pretrained VGG16 with customized upsampling layers\n",
        "class VGGColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(VGGColorizationNet, self).__init__()\n",
        "    CONV_FEATURE_SIZE = 512\n",
        "\n",
        "    ## First half: ResNet\n",
        "    vggnet = models.vgg16_bn(num_classes=365) \n",
        "    # Extract convolutional downsampling layers architecture\n",
        "    self.conv_vggnet = nn.Sequential(*list(vggnet.children())[:2])\n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    self.conv_vggnet[0][0].weight=nn.Parameter(self.conv_vggnet[0][0].weight.sum(dim=1).unsqueeze(1)) \n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(\n",
        "          # nn.ReLU(),\n",
        "          nn.ConvTranspose2d(CONV_FEATURE_SIZE, 256, kernel_size=4, stride=2, padding=1),\n",
        "          nn.BatchNorm2d(256),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "          nn.Sigmoid(),\n",
        "          nn.ConvTranspose2d(32, 2, kernel_size=4, stride=2, padding=1),\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through ResNet-gray to extract features\n",
        "    conv_features = self.conv_vggnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(conv_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "FaE52sT5stXi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customized dataloader to apply transformation and convert RGB images into Lab\n",
        "class GrayscaleImageFolder(datasets.ImageFolder):\n",
        "  '''Custom images folder, which converts images to grayscale before loading'''\n",
        "  def __getitem__(self, index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is not None:\n",
        "      img_original = self.transform(img)\n",
        "      img_original = np.asarray(img_original)\n",
        "      img_lab = rgb2lab(img_original)\n",
        "      img_lab = (img_lab + 128) / 255\n",
        "      img_ab = img_lab[:, :, 1:3]\n",
        "      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "      img_original = rgb2gray(img_original)\n",
        "      img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "    return img_original, img_ab, target"
      ],
      "metadata": {
        "id": "GsBPtreeNnUF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training helper\n",
        "class AverageMeter(object):\n",
        "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "\n",
        "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
        "  '''Show/save rgb image from grayscale and ab channels\n",
        "     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
        "  plt.clf() # clear matplotlib \n",
        "  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n",
        "  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
        "  color_image = lab2rgb(color_image.astype(np.float64))\n",
        "  grayscale_input = grayscale_input.squeeze().numpy()\n",
        "  if save_path is not None and save_name is not None: \n",
        "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
        "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
      ],
      "metadata": {
        "id": "RoXFh-TJRDgB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Validation\n",
        "def validate(val_loader, model, criterion, save_images, epoch):\n",
        "  model.eval()\n",
        "\n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  already_saved_images = False\n",
        "  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Use GPU\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Run model and record loss\n",
        "    output_ab = model(input_gray) # throw away class predictions\n",
        "    loss = criterion(output_ab, input_ab)\n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Save images to file\n",
        "    if save_images and not already_saved_images:\n",
        "      already_saved_images = True\n",
        "      for j in range(min(len(output_ab), 10)): # save at most 5 images\n",
        "        save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n",
        "        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
        "        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
        "\n",
        "    # Record time to do forward passes and save images\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
        "    if i % 25 == 0:\n",
        "      print('Validate: [{0}/{1}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
        "\n",
        "  print('Finished validation.')\n",
        "  return losses.avg"
      ],
      "metadata": {
        "id": "bAMFJOaQRHy1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "  print('Starting training epoch {}'.format(epoch))\n",
        "  model.train()\n",
        "  \n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n",
        "    \n",
        "    # Use GPU if available\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Record time to load data (above)\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Run forward pass\n",
        "    output_ab = model(input_gray) \n",
        "    loss = criterion(output_ab, input_ab) \n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Compute gradient and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Record time to do forward and backward passes\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
        "    if i % 25 == 0:\n",
        "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "              epoch, i, len(train_loader), batch_time=batch_time,\n",
        "             data_time=data_time, loss=losses)) \n",
        "\n",
        "  print('Finished training epoch {}'.format(epoch))\n",
        "  return losses.avg"
      ],
      "metadata": {
        "id": "ssHMWsHtRIn5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which architecture to use\n",
        "# candidate CNN structures\n",
        "cnn_str = \"ResNet\""
      ],
      "metadata": {
        "id": "80e_YXDeopJs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "if cnn_str == \"ResNet\":\n",
        "  model = ResColorizationNet()\n",
        "elif cnn_str == \"AlexNet\":\n",
        "  model = AlexColorizationNet()\n",
        "elif cnn_str == \"VGGNet\":\n",
        "  model = VGGColorizationNet()"
      ],
      "metadata": {
        "id": "zeTawOuYo4fA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick optimizer and loss functions\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "metadata": {
        "id": "fFFBw_jIo62s"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if cnn_str == \"AlexNet\":\n",
        "  resize = 227\n",
        "else:\n",
        "  resize = 224\n",
        "\n",
        "# Training data loader\n",
        "train_transforms = transforms.Compose([transforms.Resize(resize)])\n",
        "train_imagefolder = GrayscaleImageFolder(\"/content/tiny-imagenet-200/train/\", train_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64,shuffle=False,sampler=torch.utils.data.SubsetRandomSampler(range(10000)))\n",
        "\n",
        "# Validation  data loader\n",
        "val_transforms = transforms.Compose([transforms.Resize(resize)])\n",
        "val_imagefolder = GrayscaleImageFolder(\"/content/tiny-imagenet-200/val/\" , val_transforms)\n",
        "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False,sampler=torch.utils.data.SubsetRandomSampler(range(1000)))\n"
      ],
      "metadata": {
        "id": "EnpypMb_opqe"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model and loss function to GPU\n",
        "use_gpu = True\n",
        "if use_gpu: \n",
        "  criterion = criterion.cuda()\n",
        "  model = model.cuda()"
      ],
      "metadata": {
        "id": "HQfthVLZRNKy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up previous created folders\n",
        "shutil.rmtree('outputs/color')\n",
        "shutil.rmtree('outputs/gray/')\n",
        "shutil.rmtree('checkpoints/')"
      ],
      "metadata": {
        "id": "Tstab64tu9p0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make folders and set parameters\n",
        "os.makedirs('outputs/color', exist_ok=True)\n",
        "os.makedirs('outputs/gray', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "save_images = True\n",
        "best_losses = 1e10\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "sMaVUNteRRUq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.image import NonUniformImage\n",
        "\n",
        "# Train model\n",
        "train_loss =[]\n",
        "val_loss = []\n",
        "best_model = None\n",
        "for epoch in range(epochs):\n",
        "  # Train for one epoch, then validate\n",
        "  train_losses=train(train_loader, model, criterion, optimizer, epoch)\n",
        "  with torch.no_grad():\n",
        "    losses = validate(val_loader, model, criterion, save_images, epoch)\n",
        "  scheduler.step()\n",
        "  val_loss.append(losses)\n",
        "  train_loss.append(train_losses)\n",
        "\n",
        "  # update best model information\n",
        "  if losses < best_losses:\n",
        "    best_losses = losses\n",
        "    best_model = model\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model.state_dict(), 'checkpoints/{}-model-losses-{:.3f}.pth'.format(cnn_str,best_losses))\n"
      ],
      "metadata": {
        "id": "dN0mKvBuRYq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# output loss report\n",
        "pd.DataFrame({'Epoch':pd.Series(range(epochs)),'Training Loss':pd.Series(train_loss),'Validation Loss':pd.Series(val_loss)}).to_csv(\"MSE.csv\")"
      ],
      "metadata": {
        "id": "5a2gn5ap0tm4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(epochs), train_loss, 'r--')\n",
        "plt.plot(range(epochs), val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(cnn_str+' Based Model Loss vs epoch using MSE')\n",
        "plt.show();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "O97TRouExM3a",
        "outputId": "3ceec35a-3713-4ff4-9160-c928fcb2e1ae"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/color.zip /content/outputs/color\n",
        "!zip -r /content/gray.zip /content/outputs/gray\n",
        "from google.colab import files\n",
        "files.download(\"/content/color.zip\")\n",
        "files.download(\"/content/gray.zip\")"
      ],
      "metadata": {
        "id": "vZr5HhT61XPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video colorization\n",
        "def frame_as_rgb(grayscale_input, ab_input):\n",
        "  color_image = torch.hstack((grayscale_input, ab_input)).squeeze(0) # combine channels\n",
        "  color_image = color_image.permute(1, 2, 0)  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
        "  color_image = lab2rgb(color_image.cpu().detach().numpy().astype(np.float64))\n",
        "  return color_image\n"
      ],
      "metadata": {
        "id": "e2MQoOL-Zj0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/mickymouse.mp4\")\n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "# Get original video features\n",
        "FPS = cap.get(cv2.CAP_PROP_FPS) #number of frames/photos taken per second by the camera\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) #check resolution\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# setup video writer\n",
        "out = cv2.VideoWriter(\"/content/mickymouse_colorized.mp4\", fourcc, FPS, (w,h), True)\n",
        "\n",
        "f=0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yEEwwqtpZvBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/video_frame/frame/', exist_ok=True)"
      ],
      "metadata": {
        "id": "VAt4V4afbVOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "# Initialize model\n",
        "if cnn_str == \"ResNet\":\n",
        "  model = ResColorizationNet()\n",
        "elif cnn_str == \"AlexNet\":\n",
        "  model = AlexColorizationNet()\n",
        "elif cnn_str == \"VGGNet\":\n",
        "  model = VGGColorizationNet()\n",
        "use_gpu = True\n",
        "if use_gpu: \n",
        "  model = model.cuda()\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/model-epoch-8-losses-0.047.pth\",\\\n",
        "                                         map_location=device)) # load appropriate model"
      ],
      "metadata": {
        "id": "3loDoiyZZ-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "OBiun9f3bxZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(resize)])"
      ],
      "metadata": {
        "id": "gydwTL_waVoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "interim_plot=False"
      ],
      "metadata": {
        "id": "TsxhU0sUi7E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while cap.isOpened():\n",
        "\n",
        "  f+=1\n",
        "  success, frame = cap.read()\n",
        "  if not success:\n",
        "    print(\"Finish reading all frames!\")\n",
        "    break\n",
        "  else:\n",
        "    print(\"Current on frame {}\".format(f))\n",
        "    cv2.imwrite(\"/content/video_frame/frame/temp.jpg\",cv2.resize(frame,(256,256)))\n",
        "    img = GrayscaleImageFolder(\"/content/video_frame/\",trans)\n",
        "    img_loader = torch.utils.data.DataLoader(img,batch_size=1)\n",
        "    for i, (img_gray, _, _) in enumerate(img_loader):\n",
        "      output_ab = model(img_gray.to(device)) # apply model to predict single frame\n",
        "      colored_frame = frame_as_rgb(img_gray.to(device), output_ab.to(device))*255\n",
        "      if interim_plot: # Interim frames checking\n",
        "        cv2_imshow(colored_frame)\n",
        "      out.write(cv2.resize(colored_frame,(w,h)).astype(\"uint8\"))\n",
        "\n",
        "  k = cv2.waitKey(30) & 0xFF #need to open the window(show the photo) in order to close the window\n",
        "  if k==ord('q'): #the q must be entered at the camera window instead of the terminal\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "vGi0cXawaqdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}