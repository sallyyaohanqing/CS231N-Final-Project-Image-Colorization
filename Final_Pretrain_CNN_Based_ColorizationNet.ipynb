{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Pretrain_CNN_Based_ColorizationNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained CNN-Based Image Colorization Net model"
      ],
      "metadata": {
        "id": "6bbTj2j2CDx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab file includes codes to train CNN model for Image Colorization task.\n",
        "The CNN model combines the downsampling convolutional layers from pretrained model such as ResNet18, AlexNet, and VGG16 with customized upsampling layers.\n",
        "Image data are trained using Lab channels where we use L channel as input and ab channels as target."
      ],
      "metadata": {
        "id": "1gD0lxI19aCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgement:\n",
        "The model architecture is inspired by Luke Melas-Kyriazi's blog post $\\textbf{Image Colorization with Convolutional Neural Networks}$: https://lukemelas.github.io/image-colorization.html. Parts of the codes are modified based on the codes the author shared in the blog post and Github: https://github.com/lukemelas/Automatic-Image-Colorization/."
      ],
      "metadata": {
        "id": "a99r7EeCCT00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and import libraries\n",
        "!pip install torch torchvision matplotlib numpy scikit-image pillow==4.1.1"
      ],
      "metadata": {
        "id": "aH0dC6LHNLBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/CellProfiling/HPA-Cell-Segmentation/archive/master.zip"
      ],
      "metadata": {
        "id": "ZTgtf52KPc-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow==9.0.0"
      ],
      "metadata": {
        "id": "ExoNR3xh1Rwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0cfed1f-b75f-4647-a3c6-0b5a42e55961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow==9.0.0 in /usr/local/lib/python3.7/dist-packages (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip file\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/colorization/tiny-imagenet-200.zip\", 'r')\n",
        "zip_ref.extractall(\".\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "YDIPLSOyi-jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# For conversion between RGB channel and Lab channel\n",
        "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
        "from skimage import io\n",
        "# For CNN model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "# For utilities\n",
        "import os, shutil, time"
      ],
      "metadata": {
        "id": "ZEimdho-NS03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "JebMDYYcNTWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsampling convolutional layers from Pretrained ResNet18 with customized upsampling layers\n",
        "class ResColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(ResColorizationNet, self).__init__()\n",
        "    CONV_FEATURE_SIZE = 128\n",
        "\n",
        "    ## First half: ResNet\n",
        "    resnet = models.resnet18(num_classes=365) \n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    # Extract convolutional downsampling layers architecture\n",
        "    self.conv_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(     \n",
        "      nn.Conv2d(CONV_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through ResNet-gray to extract features\n",
        "    conv_features = self.conv_resnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(conv_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "BDbj81YCU08r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsampling convolutional layers from Pretrained AlexNet with customized upsampling layers\n",
        "class AlexColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(AlexColorizationNet, self).__init__()\n",
        "    CONV_FEATURE_SIZE = 256\n",
        "\n",
        "    ## First half: Alexnet\n",
        "    alexnet = models.alexnet(num_classes=365) \n",
        "    # Extract convolutional downsampling layers architecture\n",
        "    self.conv_alexnet = nn.Sequential(*list(alexnet.children())[0][:12])\n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    self.conv_alexnet[0].weight=nn.Parameter(self.conv_alexnet[0].weight.sum(dim=1).unsqueeze(1)) \n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(  \n",
        "      nn.Conv2d(CONV_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=3),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=3),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(2),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(2, 2, kernel_size=8, stride=1, padding=0),\n",
        "      nn.LeakyReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through AlexNet-gray to extract features\n",
        "    conv_features = self.conv_alexnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(conv_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "0Bu8Nz7mNZOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsampling convolutional layers from Pretrained VGG16 with customized upsampling layers\n",
        "class VGGColorizationNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(VGGColorizationNet, self).__init__()\n",
        "    CONV_FEATURE_SIZE = 512\n",
        "\n",
        "    ## First half: ResNet\n",
        "    vggnet = models.vgg16_bn(num_classes=365) \n",
        "    # Extract convolutional downsampling layers architecture\n",
        "    self.conv_vggnet = nn.Sequential(*list(vggnet.children())[:2])\n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    self.conv_vggnet[0][0].weight=nn.Parameter(self.conv_vggnet[0][0].weight.sum(dim=1).unsqueeze(1)) \n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(\n",
        "          # nn.ReLU(),\n",
        "          nn.ConvTranspose2d(CONV_FEATURE_SIZE, 256, kernel_size=4, stride=2, padding=1),\n",
        "          nn.BatchNorm2d(256),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.LeakyReLU(),\n",
        "          nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "          nn.Sigmoid(),\n",
        "          nn.ConvTranspose2d(32, 2, kernel_size=4, stride=2, padding=1),\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through ResNet-gray to extract features\n",
        "    conv_features = self.conv_vggnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(conv_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "FaE52sT5stXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customized dataloader to apply transformation and convert RGB images into Lab\n",
        "class GrayscaleImageFolder(datasets.ImageFolder):\n",
        "  '''Custom images folder, which converts images to grayscale before loading'''\n",
        "  def __getitem__(self, index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is not None:\n",
        "      img_original = self.transform(img)\n",
        "      img_original = np.asarray(img_original)\n",
        "      img_lab = rgb2lab(img_original)\n",
        "      img_lab = (img_lab + 128) / 255\n",
        "      img_ab = img_lab[:, :, 1:3]\n",
        "      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "      img_original = rgb2gray(img_original)\n",
        "      img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "    return img_original, img_ab, target"
      ],
      "metadata": {
        "id": "GsBPtreeNnUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training helper\n",
        "class AverageMeter(object):\n",
        "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "\n",
        "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
        "  '''Show/save rgb image from grayscale and ab channels\n",
        "     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
        "  plt.clf() # clear matplotlib \n",
        "  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n",
        "  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
        "  color_image = lab2rgb(color_image.astype(np.float64))\n",
        "  grayscale_input = grayscale_input.squeeze().numpy()\n",
        "  if save_path is not None and save_name is not None: \n",
        "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
        "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
      ],
      "metadata": {
        "id": "RoXFh-TJRDgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Validation\n",
        "def validate(val_loader, model, criterion, save_images, epoch):\n",
        "  model.eval()\n",
        "\n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  already_saved_images = False\n",
        "  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Use GPU\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Run model and record loss\n",
        "    output_ab = model(input_gray) # throw away class predictions\n",
        "    loss = criterion(output_ab, input_ab)\n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Save images to file\n",
        "    if save_images and not already_saved_images:\n",
        "      already_saved_images = True\n",
        "      for j in range(min(len(output_ab), 10)): # save at most 5 images\n",
        "        save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n",
        "        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
        "        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
        "\n",
        "    # Record time to do forward passes and save images\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
        "    if i % 25 == 0:\n",
        "      print('Validate: [{0}/{1}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
        "\n",
        "  print('Finished validation.')\n",
        "  return losses.avg"
      ],
      "metadata": {
        "id": "bAMFJOaQRHy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "  print('Starting training epoch {}'.format(epoch))\n",
        "  model.train()\n",
        "  \n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n",
        "    \n",
        "    # Use GPU if available\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Record time to load data (above)\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Run forward pass\n",
        "    output_ab = model(input_gray) \n",
        "    loss = criterion(output_ab, input_ab) \n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Compute gradient and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Record time to do forward and backward passes\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
        "    if i % 25 == 0:\n",
        "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "              epoch, i, len(train_loader), batch_time=batch_time,\n",
        "             data_time=data_time, loss=losses)) \n",
        "\n",
        "  print('Finished training epoch {}'.format(epoch))\n",
        "  return losses.avg"
      ],
      "metadata": {
        "id": "ssHMWsHtRIn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which architecture to use\n",
        "# candidate CNN structures\n",
        "cnn_str = \"VGGNet\""
      ],
      "metadata": {
        "id": "80e_YXDeopJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "if cnn_str == \"ResNet\":\n",
        "  model = ResColorizationNet()\n",
        "elif cnn_str == \"AlexNet\":\n",
        "  model = AlexColorizationNet()\n",
        "elif cnn_str == \"VGGNet\":\n",
        "  model = VGGColorizationNet()"
      ],
      "metadata": {
        "id": "zeTawOuYo4fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick optimizer and loss functions\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0) # WD strength: 0 for AlexNet, 0.5 for ResNet,0.9 WD for AlexNet\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "metadata": {
        "id": "fFFBw_jIo62s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if cnn_str == \"AlexNet\":\n",
        "  resize = 227\n",
        "else:\n",
        "  resize = 224\n",
        "\n",
        "# Training data loader\n",
        "train_transforms = transforms.Compose([transforms.Resize(resize)])\n",
        "train_imagefolder = GrayscaleImageFolder(\"/content/tiny-imagenet-200/train/\", train_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64,shuffle=False,sampler=torch.utils.data.SubsetRandomSampler(range(10000)))\n",
        "\n",
        "# Validation  data loader\n",
        "val_transforms = transforms.Compose([transforms.Resize(resize)])\n",
        "val_imagefolder = GrayscaleImageFolder(\"/content/tiny-imagenet-200/val/\" , val_transforms)\n",
        "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False,sampler=torch.utils.data.SubsetRandomSampler(range(1000)))\n"
      ],
      "metadata": {
        "id": "EnpypMb_opqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model and loss function to GPU\n",
        "use_gpu = True\n",
        "if use_gpu: \n",
        "  criterion = criterion.cuda()\n",
        "  model = model.cuda()"
      ],
      "metadata": {
        "id": "HQfthVLZRNKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up previous created folders\n",
        "shutil.rmtree('outputs/color')\n",
        "shutil.rmtree('outputs/gray/')\n",
        "shutil.rmtree('checkpoints/')"
      ],
      "metadata": {
        "id": "Tstab64tu9p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make folders and set parameters\n",
        "os.makedirs('outputs/color', exist_ok=True)\n",
        "os.makedirs('outputs/gray', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "save_images = True\n",
        "best_losses = 1e10\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "sMaVUNteRRUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.image import NonUniformImage\n",
        "\n",
        "# Train model\n",
        "train_loss =[]\n",
        "val_loss = []\n",
        "best_model = None\n",
        "for epoch in range(epochs):\n",
        "  # Train for one epoch, then validate\n",
        "  train_losses=train(train_loader, model, criterion, optimizer, epoch)\n",
        "  with torch.no_grad():\n",
        "    losses = validate(val_loader, model, criterion, save_images, epoch)\n",
        "  scheduler.step()\n",
        "  val_loss.append(losses)\n",
        "  train_loss.append(train_losses)\n",
        "\n",
        "  # update best model information\n",
        "  if losses < best_losses:\n",
        "    best_losses = losses\n",
        "    best_model = model\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model.state_dict(), 'checkpoints/{}-model-losses-{:.3f}.pth'.format(cnn_str,best_losses))\n"
      ],
      "metadata": {
        "id": "dN0mKvBuRYq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58f54ab6-9d55-4f99-947a-c4764ab3bfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training epoch 0\n",
            "Epoch: [0][0/157]\tTime 1.288 (1.288)\tData 1.203 (1.203)\tLoss 0.0103 (0.0103)\t\n",
            "Epoch: [0][25/157]\tTime 1.228 (1.315)\tData 1.176 (1.261)\tLoss 0.0051 (0.0074)\t\n",
            "Epoch: [0][50/157]\tTime 1.215 (1.272)\tData 1.163 (1.219)\tLoss 0.0052 (0.0062)\t\n",
            "Epoch: [0][75/157]\tTime 1.230 (1.261)\tData 1.178 (1.209)\tLoss 0.0041 (0.0057)\t\n",
            "Epoch: [0][100/157]\tTime 1.232 (1.250)\tData 1.181 (1.198)\tLoss 0.0050 (0.0054)\t\n",
            "Epoch: [0][125/157]\tTime 1.208 (1.244)\tData 1.155 (1.192)\tLoss 0.0033 (0.0051)\t\n",
            "Epoch: [0][150/157]\tTime 1.193 (1.239)\tData 1.142 (1.187)\tLoss 0.0045 (0.0051)\t\n",
            "Finished training epoch 0\n",
            "Validate: [0/16]\tTime 1.496 (1.496)\tLoss 0.0035 (0.0035)\t\n",
            "Finished validation.\n",
            "Starting training epoch 1\n",
            "Epoch: [1][0/157]\tTime 1.281 (1.281)\tData 1.197 (1.197)\tLoss 0.0050 (0.0050)\t\n",
            "Epoch: [1][25/157]\tTime 1.221 (1.258)\tData 1.170 (1.206)\tLoss 0.0048 (0.0044)\t\n",
            "Epoch: [1][50/157]\tTime 1.210 (1.237)\tData 1.160 (1.185)\tLoss 0.0046 (0.0044)\t\n",
            "Epoch: [1][75/157]\tTime 1.199 (1.226)\tData 1.149 (1.174)\tLoss 0.0034 (0.0044)\t\n",
            "Epoch: [1][100/157]\tTime 1.222 (1.219)\tData 1.172 (1.168)\tLoss 0.0054 (0.0045)\t\n",
            "Epoch: [1][125/157]\tTime 1.217 (1.216)\tData 1.164 (1.165)\tLoss 0.0030 (0.0045)\t\n",
            "Epoch: [1][150/157]\tTime 1.212 (1.214)\tData 1.162 (1.162)\tLoss 0.0071 (0.0046)\t\n",
            "Finished training epoch 1\n",
            "Validate: [0/16]\tTime 1.481 (1.481)\tLoss 0.0042 (0.0042)\t\n",
            "Finished validation.\n",
            "Starting training epoch 2\n",
            "Epoch: [2][0/157]\tTime 1.317 (1.317)\tData 1.233 (1.233)\tLoss 0.0045 (0.0045)\t\n",
            "Epoch: [2][25/157]\tTime 1.208 (1.199)\tData 1.157 (1.147)\tLoss 0.0048 (0.0046)\t\n",
            "Epoch: [2][50/157]\tTime 1.196 (1.203)\tData 1.146 (1.152)\tLoss 0.0064 (0.0047)\t\n",
            "Epoch: [2][75/157]\tTime 1.234 (1.205)\tData 1.181 (1.153)\tLoss 0.0050 (0.0047)\t\n",
            "Epoch: [2][100/157]\tTime 1.212 (1.207)\tData 1.158 (1.156)\tLoss 0.0040 (0.0046)\t\n",
            "Epoch: [2][125/157]\tTime 1.206 (1.217)\tData 1.154 (1.166)\tLoss 0.0044 (0.0046)\t\n",
            "Epoch: [2][150/157]\tTime 1.194 (1.214)\tData 1.143 (1.162)\tLoss 0.0030 (0.0046)\t\n",
            "Finished training epoch 2\n",
            "Validate: [0/16]\tTime 1.472 (1.472)\tLoss 0.0035 (0.0035)\t\n",
            "Finished validation.\n",
            "Starting training epoch 3\n",
            "Epoch: [3][0/157]\tTime 1.246 (1.246)\tData 1.161 (1.161)\tLoss 0.0038 (0.0038)\t\n",
            "Epoch: [3][25/157]\tTime 1.218 (1.205)\tData 1.167 (1.152)\tLoss 0.0032 (0.0048)\t\n",
            "Epoch: [3][50/157]\tTime 1.184 (1.200)\tData 1.133 (1.148)\tLoss 0.0034 (0.0047)\t\n",
            "Epoch: [3][75/157]\tTime 1.195 (1.199)\tData 1.143 (1.148)\tLoss 0.0052 (0.0047)\t\n",
            "Epoch: [3][100/157]\tTime 1.204 (1.199)\tData 1.154 (1.147)\tLoss 0.0053 (0.0046)\t\n",
            "Epoch: [3][125/157]\tTime 1.192 (1.198)\tData 1.140 (1.146)\tLoss 0.0041 (0.0045)\t\n",
            "Epoch: [3][150/157]\tTime 1.170 (1.197)\tData 1.116 (1.145)\tLoss 0.0043 (0.0046)\t\n",
            "Finished training epoch 3\n",
            "Validate: [0/16]\tTime 1.495 (1.495)\tLoss 0.0037 (0.0037)\t\n",
            "Finished validation.\n",
            "Starting training epoch 4\n",
            "Epoch: [4][0/157]\tTime 1.250 (1.250)\tData 1.165 (1.165)\tLoss 0.0029 (0.0029)\t\n",
            "Epoch: [4][25/157]\tTime 1.249 (1.237)\tData 1.198 (1.184)\tLoss 0.0042 (0.0043)\t\n",
            "Epoch: [4][50/157]\tTime 1.182 (1.213)\tData 1.132 (1.161)\tLoss 0.0028 (0.0043)\t\n",
            "Epoch: [4][75/157]\tTime 1.168 (1.204)\tData 1.117 (1.153)\tLoss 0.0062 (0.0045)\t\n",
            "Epoch: [4][100/157]\tTime 1.193 (1.201)\tData 1.143 (1.150)\tLoss 0.0035 (0.0045)\t\n",
            "Epoch: [4][125/157]\tTime 1.204 (1.201)\tData 1.153 (1.150)\tLoss 0.0044 (0.0046)\t\n",
            "Epoch: [4][150/157]\tTime 1.216 (1.201)\tData 1.165 (1.150)\tLoss 0.0041 (0.0046)\t\n",
            "Finished training epoch 4\n",
            "Validate: [0/16]\tTime 1.487 (1.487)\tLoss 0.0030 (0.0030)\t\n",
            "Finished validation.\n",
            "Starting training epoch 5\n",
            "Epoch: [5][0/157]\tTime 1.273 (1.273)\tData 1.189 (1.189)\tLoss 0.0062 (0.0062)\t\n",
            "Epoch: [5][25/157]\tTime 1.187 (1.195)\tData 1.136 (1.143)\tLoss 0.0041 (0.0045)\t\n",
            "Epoch: [5][50/157]\tTime 1.198 (1.194)\tData 1.147 (1.143)\tLoss 0.0040 (0.0046)\t\n",
            "Epoch: [5][75/157]\tTime 1.211 (1.192)\tData 1.160 (1.141)\tLoss 0.0047 (0.0046)\t\n",
            "Epoch: [5][100/157]\tTime 1.193 (1.193)\tData 1.144 (1.142)\tLoss 0.0042 (0.0046)\t\n",
            "Epoch: [5][125/157]\tTime 1.189 (1.203)\tData 1.138 (1.151)\tLoss 0.0045 (0.0046)\t\n",
            "Epoch: [5][150/157]\tTime 1.212 (1.202)\tData 1.161 (1.151)\tLoss 0.0040 (0.0046)\t\n",
            "Finished training epoch 5\n",
            "Validate: [0/16]\tTime 1.479 (1.479)\tLoss 0.0029 (0.0029)\t\n",
            "Finished validation.\n",
            "Starting training epoch 6\n",
            "Epoch: [6][0/157]\tTime 1.241 (1.241)\tData 1.156 (1.156)\tLoss 0.0040 (0.0040)\t\n",
            "Epoch: [6][25/157]\tTime 1.207 (1.193)\tData 1.155 (1.140)\tLoss 0.0037 (0.0046)\t\n",
            "Epoch: [6][50/157]\tTime 1.185 (1.192)\tData 1.134 (1.141)\tLoss 0.0060 (0.0047)\t\n",
            "Epoch: [6][75/157]\tTime 1.192 (1.193)\tData 1.140 (1.142)\tLoss 0.0051 (0.0047)\t\n",
            "Epoch: [6][100/157]\tTime 1.177 (1.193)\tData 1.126 (1.141)\tLoss 0.0044 (0.0047)\t\n",
            "Epoch: [6][125/157]\tTime 1.197 (1.191)\tData 1.146 (1.140)\tLoss 0.0062 (0.0047)\t\n",
            "Epoch: [6][150/157]\tTime 1.199 (1.192)\tData 1.149 (1.141)\tLoss 0.0047 (0.0046)\t\n",
            "Finished training epoch 6\n",
            "Validate: [0/16]\tTime 1.469 (1.469)\tLoss 0.0049 (0.0049)\t\n",
            "Finished validation.\n",
            "Starting training epoch 7\n",
            "Epoch: [7][0/157]\tTime 1.243 (1.243)\tData 1.159 (1.159)\tLoss 0.0037 (0.0037)\t\n",
            "Epoch: [7][25/157]\tTime 1.208 (1.234)\tData 1.157 (1.181)\tLoss 0.0051 (0.0048)\t\n",
            "Epoch: [7][50/157]\tTime 1.205 (1.216)\tData 1.155 (1.164)\tLoss 0.0047 (0.0046)\t\n",
            "Epoch: [7][75/157]\tTime 1.182 (1.210)\tData 1.131 (1.158)\tLoss 0.0060 (0.0046)\t\n",
            "Epoch: [7][100/157]\tTime 1.190 (1.207)\tData 1.139 (1.156)\tLoss 0.0049 (0.0047)\t\n",
            "Epoch: [7][125/157]\tTime 1.178 (1.206)\tData 1.127 (1.154)\tLoss 0.0033 (0.0046)\t\n",
            "Epoch: [7][150/157]\tTime 1.175 (1.203)\tData 1.125 (1.152)\tLoss 0.0034 (0.0047)\t\n",
            "Finished training epoch 7\n",
            "Validate: [0/16]\tTime 1.457 (1.457)\tLoss 0.0049 (0.0049)\t\n",
            "Finished validation.\n",
            "Starting training epoch 8\n",
            "Epoch: [8][0/157]\tTime 1.236 (1.236)\tData 1.152 (1.152)\tLoss 0.0055 (0.0055)\t\n",
            "Epoch: [8][25/157]\tTime 1.202 (1.202)\tData 1.152 (1.149)\tLoss 0.0040 (0.0051)\t\n",
            "Epoch: [8][50/157]\tTime 1.184 (1.201)\tData 1.135 (1.149)\tLoss 0.0049 (0.0048)\t\n",
            "Epoch: [8][75/157]\tTime 1.309 (1.205)\tData 1.259 (1.154)\tLoss 0.0054 (0.0047)\t\n",
            "Epoch: [8][100/157]\tTime 1.210 (1.218)\tData 1.158 (1.167)\tLoss 0.0039 (0.0047)\t\n",
            "Epoch: [8][125/157]\tTime 1.195 (1.215)\tData 1.145 (1.164)\tLoss 0.0030 (0.0047)\t\n",
            "Epoch: [8][150/157]\tTime 1.208 (1.213)\tData 1.158 (1.162)\tLoss 0.0066 (0.0046)\t\n",
            "Finished training epoch 8\n",
            "Validate: [0/16]\tTime 1.477 (1.477)\tLoss 0.0031 (0.0031)\t\n",
            "Finished validation.\n",
            "Starting training epoch 9\n",
            "Epoch: [9][0/157]\tTime 1.238 (1.238)\tData 1.155 (1.155)\tLoss 0.0057 (0.0057)\t\n",
            "Epoch: [9][25/157]\tTime 1.199 (1.201)\tData 1.149 (1.149)\tLoss 0.0040 (0.0046)\t\n",
            "Epoch: [9][50/157]\tTime 1.248 (1.207)\tData 1.197 (1.155)\tLoss 0.0037 (0.0045)\t\n",
            "Epoch: [9][75/157]\tTime 1.242 (1.215)\tData 1.191 (1.163)\tLoss 0.0054 (0.0046)\t\n",
            "Epoch: [9][100/157]\tTime 1.177 (1.212)\tData 1.126 (1.160)\tLoss 0.0040 (0.0046)\t\n",
            "Epoch: [9][125/157]\tTime 1.211 (1.208)\tData 1.160 (1.157)\tLoss 0.0034 (0.0046)\t\n",
            "Epoch: [9][150/157]\tTime 1.202 (1.206)\tData 1.148 (1.155)\tLoss 0.0056 (0.0046)\t\n",
            "Finished training epoch 9\n",
            "Validate: [0/16]\tTime 1.456 (1.456)\tLoss 0.0034 (0.0034)\t\n",
            "Finished validation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# output loss report\n",
        "pd.DataFrame({'Epoch':pd.Series(range(epochs)),'Training Loss':pd.Series(train_loss),'Validation Loss':pd.Series(val_loss)}).to_csv(\"MSE.csv\")"
      ],
      "metadata": {
        "id": "5a2gn5ap0tm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(epochs), train_loss, 'r--')\n",
        "plt.plot(range(epochs), val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(cnn_str+' Based Model Loss vs epoch using MSE')\n",
        "plt.show();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "O97TRouExM3a",
        "outputId": "d32eb985-c719-47d1-cead-be6e12be5540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/color.zip /content/outputs/color\n",
        "!zip -r /content/gray.zip /content/outputs/gray\n",
        "from google.colab import files\n",
        "files.download(\"/content/color.zip\")\n",
        "files.download(\"/content/gray.zip\")"
      ],
      "metadata": {
        "id": "vZr5HhT61XPH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc89240f-2f75-4bbb-f8e4-6b6f42f45167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/outputs/color/ (stored 0%)\n",
            "updating: content/outputs/color/img-9-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-0-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-9-epoch-4.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-4-epoch-8.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-9-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-9-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-6.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-6-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-3-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-2-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-3-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-0-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-2-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-2-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-1.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-8-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-6-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-3-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-2-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-5-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-0-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-3-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-3-epoch-9.jpg (deflated 4%)\n",
            "updating: content/outputs/color/img-8-epoch-9.jpg (deflated 5%)\n",
            "updating: content/outputs/color/img-2-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-7-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-6-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-9-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-3-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-6-epoch-4.jpg (deflated 1%)\n",
            "updating: content/outputs/color/img-7-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-9-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-4.jpg (deflated 1%)\n",
            "updating: content/outputs/color/img-2-epoch-9.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-8-epoch-0.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-2-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-2-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-4-epoch-1.jpg (deflated 1%)\n",
            "updating: content/outputs/color/img-0-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-9-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-0-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-3-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-0-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-6.jpg (deflated 1%)\n",
            "updating: content/outputs/color/img-6-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-5-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-0.jpg (deflated 10%)\n",
            "updating: content/outputs/color/img-9-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-3.jpg (deflated 4%)\n",
            "updating: content/outputs/color/img-5-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-8.jpg (deflated 5%)\n",
            "updating: content/outputs/color/img-0-epoch-4.jpg (deflated 1%)\n",
            "updating: content/outputs/color/img-6-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-6-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-3-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-3-epoch-5.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-4-epoch-2.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-6-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-6-epoch-1.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-2-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-5-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-6.jpg (deflated 1%)\n",
            "updating: content/outputs/color/img-0-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-7-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-8-epoch-8.jpg (deflated 4%)\n",
            "updating: content/outputs/color/img-1-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-4-epoch-5.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-3-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-9-epoch-2.jpg (deflated 5%)\n",
            "updating: content/outputs/color/img-0-epoch-8.jpg (deflated 3%)\n",
            "updating: content/outputs/color/img-1-epoch-8.jpg (deflated 4%)\n",
            "updating: content/outputs/color/img-6-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-1-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-9-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-2-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/color/img-0-epoch-0.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/ (stored 0%)\n",
            "updating: content/outputs/gray/img-9-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-0-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-9-epoch-4.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-4-epoch-8.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-9-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-9-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-6.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-6-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-3-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-3-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-4-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-8-epoch-4.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-7-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-0-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-8-epoch-1.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-8-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-4-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-8-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-6-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-8-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-3-epoch-8.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-8-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-1-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-5-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-4-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-0-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-3-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-3-epoch-9.jpg (deflated 4%)\n",
            "updating: content/outputs/gray/img-8-epoch-9.jpg (deflated 4%)\n",
            "updating: content/outputs/gray/img-2-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-6-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-9-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-3-epoch-1.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-6-epoch-4.jpg (deflated 1%)\n",
            "updating: content/outputs/gray/img-7-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-9-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-4-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-9.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-8-epoch-0.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-2-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-4-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-0-epoch-9.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-9-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-0-epoch-3.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-3-epoch-2.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-0-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-8-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-6-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-5-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-0.jpg (deflated 12%)\n",
            "updating: content/outputs/gray/img-9-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-4-epoch-3.jpg (deflated 5%)\n",
            "updating: content/outputs/gray/img-5-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-8.jpg (deflated 5%)\n",
            "updating: content/outputs/gray/img-0-epoch-4.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-6-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-6-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-3-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-4-epoch-7.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-3-epoch-5.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-4-epoch-2.jpg (deflated 4%)\n",
            "updating: content/outputs/gray/img-6-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-6-epoch-1.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-2-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-5-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-0-epoch-1.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-7-epoch-3.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-8-epoch-8.jpg (deflated 4%)\n",
            "updating: content/outputs/gray/img-1-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-4-epoch-5.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-3-epoch-7.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-9-epoch-2.jpg (deflated 5%)\n",
            "updating: content/outputs/gray/img-0-epoch-8.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-1-epoch-8.jpg (deflated 3%)\n",
            "updating: content/outputs/gray/img-6-epoch-0.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-6.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-1-epoch-2.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-9-epoch-5.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-2-epoch-8.jpg (deflated 2%)\n",
            "updating: content/outputs/gray/img-0-epoch-0.jpg (deflated 4%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0d5c37ca-2d50-4eb9-b692-7e1b2c6f367e\", \"color.zip\", 699950)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0b0cf3c1-6600-4419-a57a-cccfb91094f2\", \"gray.zip\", 727075)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Video colorization\n",
        "def frame_as_rgb(grayscale_input, ab_input):\n",
        "  color_image = torch.hstack((grayscale_input, ab_input)).squeeze(0) # combine channels\n",
        "  color_image = color_image.permute(1, 2, 0)  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
        "  color_image = lab2rgb(color_image.cpu().detach().numpy().astype(np.float64))\n",
        "  return color_image\n"
      ],
      "metadata": {
        "id": "e2MQoOL-Zj0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/mickymouse.mp4\")\n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "# Get original video features\n",
        "FPS = cap.get(cv2.CAP_PROP_FPS) #number of frames/photos taken per second by the camera\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) #check resolution\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# setup video writer\n",
        "out = cv2.VideoWriter(\"/content/mickymouse_colorized.mp4\", fourcc, FPS, (w,h), True)\n",
        "\n",
        "f=0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yEEwwqtpZvBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/video_frame/frame/', exist_ok=True)"
      ],
      "metadata": {
        "id": "VAt4V4afbVOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "# Initialize model\n",
        "if cnn_str == \"ResNet\":\n",
        "  model = ResColorizationNet()\n",
        "elif cnn_str == \"AlexNet\":\n",
        "  model = AlexColorizationNet()\n",
        "elif cnn_str == \"VGGNet\":\n",
        "  model = VGGColorizationNet()\n",
        "use_gpu = True\n",
        "if use_gpu: \n",
        "  model = model.cuda()\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/model-epoch-8-losses-0.047.pth\",\\\n",
        "                                         map_location=device)) # load appropriate model"
      ],
      "metadata": {
        "id": "3loDoiyZZ-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "OBiun9f3bxZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(resize)])"
      ],
      "metadata": {
        "id": "gydwTL_waVoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "interim_plot=False"
      ],
      "metadata": {
        "id": "TsxhU0sUi7E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while cap.isOpened():\n",
        "\n",
        "  f+=1\n",
        "  success, frame = cap.read()\n",
        "  if not success:\n",
        "    print(\"Finish reading all frames!\")\n",
        "    break\n",
        "  else:\n",
        "    print(\"Current on frame {}\".format(f))\n",
        "    cv2.imwrite(\"/content/video_frame/frame/temp.jpg\",cv2.resize(frame,(256,256)))\n",
        "    img = GrayscaleImageFolder(\"/content/video_frame/\",trans)\n",
        "    img_loader = torch.utils.data.DataLoader(img,batch_size=1)\n",
        "    for i, (img_gray, _, _) in enumerate(img_loader):\n",
        "      output_ab = model(img_gray.to(device)) # apply model to predict single frame\n",
        "      colored_frame = frame_as_rgb(img_gray.to(device), output_ab.to(device))*255\n",
        "      if interim_plot: # Interim frames checking\n",
        "        cv2_imshow(colored_frame)\n",
        "      out.write(cv2.resize(colored_frame,(w,h)).astype(\"uint8\"))\n",
        "\n",
        "  k = cv2.waitKey(30) & 0xFF #need to open the window(show the photo) in order to close the window\n",
        "  if k==ord('q'): #the q must be entered at the camera window instead of the terminal\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "vGi0cXawaqdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}